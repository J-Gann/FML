{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base classes\n",
    "\n",
    "class Node:\n",
    "    pass\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "    \n",
    "    def find_leaf(self, x):\n",
    "        node = self.root\n",
    "        while hasattr(node, \"feature\"):\n",
    "            j = node.feature\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DensityTree, self).__init__()\n",
    "        \n",
    "    def train(self, data, prior, n_min=20):\n",
    "        '''\n",
    "        data: the feature matrix for the digit under consideration\n",
    "        prior: the prior probability of this digit\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        self.prior = prior\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D)) # number of features to consider for each split decision\n",
    "\n",
    "        # find and remember the tree's bounding box, \n",
    "        # i.e. the lower and upper limits of the training feature set\n",
    "        m, M = np.min(data, axis=0), np.max(data, axis=0)\n",
    "        self.box = m.copy(), M.copy()\n",
    "        \n",
    "        # identify invalid features and adjust the bounding box\n",
    "        # (If m[j] == M[j] for some j, the bounding box has zero volume, \n",
    "        #  causing divide-by-zero errors later on. We must exclude these\n",
    "        #  features from splitting and adjust the bounding box limits \n",
    "        #  such that invalid features have no effect on the volume.)\n",
    "        valid_features   = np.where(m != M)[0]\n",
    "        invalid_features = np.where(m == M)[0]\n",
    "        M[invalid_features] = m[invalid_features] + 1\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.box = m.copy(), M.copy()\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0] # number of instances in present node\n",
    "            if n >= n_min:\n",
    "\n",
    "                # Call 'make_density_split_node()' with 'D_try' randomly selected \n",
    "                # indices from 'valid_features'. This turns 'node' into a split node\n",
    "                # and returns the two children, which must be placed on the 'stack'.\n",
    "                feature_indices = np.random.permutation(np.arange(D))[:D_try]\n",
    "                left, right = make_density_split_node(node, N, feature_indices)\n",
    "                stack += [left, right]\n",
    "            else:\n",
    "                # Call 'make_density_leaf_node()' to turn 'node' into a leaf node.\n",
    "                make_density_leaf_node(node, N)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        m, M = self.box\n",
    "        \n",
    "        if np.any(x > M) or np.any(x < m):\n",
    "            return 0\n",
    "\n",
    "        leaf = self.find_leaf(x)\n",
    "        # return p(x | y) * p(y) if x is within the tree's bounding box \n",
    "        # and return 0 otherwise\n",
    "        return leaf.response * self.prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Did not limit the random permuation of features to the previously computed set of valid features</div>\n",
    "<div style=\"color: green; font-weight: bold\">Prediction is very similar to sample solution</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_density_split_node(node, N, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    N:    the total number of training instances for the current class\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "    m, M = node.box\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    e_min = float(\"inf\")\n",
    "    \n",
    "    for feat in feature_indices:\n",
    "        # Hint: For each feature considered, first remove duplicate feature values using \n",
    "        # 'np.unique()'. Describe here why this is necessary.\n",
    "        # It is necessary because otherwise we would recieve a threshold candidates\n",
    "        # on a feature value not between two of them\n",
    "        data_unique = np.sort(np.unique(node.data[:, feat]), axis=0)\n",
    "        # Compute candidate thresholds\n",
    "        tj = (data_unique[1:] + data_unique[:-1]) / 2\n",
    "        \n",
    "        for t in tj:\n",
    "            loo_error = ( leave_one_out_error(node.data, node.data[node.data[:, feat] <= t], t, N)\n",
    "                        + leave_one_out_error(node.data, node.data[node.data[:, feat] > t], t, N))\n",
    "\n",
    "            # choose the best threshold that\n",
    "            if loo_error < e_min:\n",
    "                e_min = loo_error\n",
    "                node.feature = feat\n",
    "                node.threshold = t\n",
    "\n",
    "    # create children\n",
    "    node.left = Node()\n",
    "    node.right = Node()\n",
    "    \n",
    "    # initialize 'left' and 'right' with the data subsets and bounding boxes\n",
    "    # according to the optimal split found above\n",
    "    is_left = node.data[:, node.feature] < node.threshold\n",
    "    node.left.data = node.data[is_left]\n",
    "    node.left.box = (node.left.data.min(axis=0), node.left.data.max(axis=0))\n",
    "    node.right.data = node.data[~is_left]\n",
    "    node.right.box = (node.right.data.min(axis=0), node.right.data.max(axis=0))\n",
    "    \n",
    "    return node.left, node.right\n",
    "\n",
    "\n",
    "def leave_one_out_error(data, data_below_threshold, t, N):\n",
    "    N_l = data_below_threshold.shape[0]\n",
    "    \n",
    "    M_l = data_below_threshold.max(axis=0)\n",
    "    m_l = data_below_threshold.min(axis=0)\n",
    "    \n",
    "    # sadly this also works to fix the box of the node\n",
    "    M_l += M_l == m_l\n",
    "\n",
    "    V_l = np.product(M_l - m_l)\n",
    "    return (N_l / (N * V_l)) * ((N_l / N) - 2 * (N_l - 1) / (N - 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Computation of candidate thresholds very similar to sample solution</div>\n",
    "<div style=\"color: green; font-weight: bold\">Leave one out error implemented in helper function, small differences in computation of required variables</div>\n",
    "<div style=\"color: green; font-weight: bold\">Initialization of left and right nodes also similar to sample solution</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_density_leaf_node(node, N):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    N:    the total number of training instances for the current class\n",
    "    '''\n",
    "    # compute and store leaf response\n",
    "    n = node.data.shape[0]\n",
    "    node.response = n / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Did not divide by volume</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DecisionTree, self).__init__()\n",
    "        \n",
    "    def train(self, data, labels, n_min=20):\n",
    "        '''\n",
    "        data: the feature matrix for all digits\n",
    "        labels: the corresponding ground-truth responses\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D)) # how many features to consider for each split decision\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.labels = labels\n",
    "        \n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0] # number of instances in present node\n",
    "            if n >= n_min and not node_is_pure(node):\n",
    "                # Call 'make_decision_split_node()' with 'D_try' randomly selected \n",
    "                # feature indices. This turns 'node' into a split node\n",
    "                # and returns the two children, which must be placed on the 'stack'.\n",
    "                feature_indices = np.random.permutation(np.arange(D))[:D_try]\n",
    "                left, right = make_decision_split_node(node, feature_indices)\n",
    "                stack += [left, right]\n",
    "            else:\n",
    "                # Call 'make_decision_leaf_node()' to turn 'node' into a leaf node.\n",
    "                make_decision_leaf_node(node)\n",
    "        \n",
    "        return self\n",
    "                \n",
    "    def predict(self, x):\n",
    "        leaf = self.find_leaf(x)\n",
    "        # compute p(y | x)\n",
    "        return leaf.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Very similar to sample solution</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decision_split_node(node, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    e_min = float(\"inf\")\n",
    "    node.feature = None\n",
    "    node.threshold = None\n",
    "    \n",
    "    for feat in feature_indices:\n",
    "        data_unique = np.sort(np.unique(node.data[:, feat]), axis=0)\n",
    "\n",
    "        # Compute candidate thresholds\n",
    "        tj = (data_unique[1:] + data_unique[:-1]) / 2\n",
    "        \n",
    "        for t in tj:\n",
    "            gi_error = (gini_impurity(node.labels[node.data[:, feat] <= t])\n",
    "                      + gini_impurity(node.labels[node.data[:, feat] > t]))\n",
    "            \n",
    "            if gi_error < e_min:\n",
    "                e_min = gi_error\n",
    "                node.feature = feat\n",
    "                node.threshold = t\n",
    "    \n",
    "    # create children\n",
    "    node.left = Node()\n",
    "    node.right = Node()\n",
    "    \n",
    "    # initialize 'left' and 'right' with the data subsets and labels\n",
    "    # according to the optimal split found above    \n",
    "    is_left = node.data[:, node.feature] < node.threshold\n",
    "    node.left.data = node.data[is_left]\n",
    "    node.left.labels = node.labels[is_left]\n",
    "    node.right.data = node.data[~is_left]\n",
    "    node.right.labels = node.labels[~is_left]\n",
    "    \n",
    "    # return the children (to be placed on the stack)\n",
    "    return node.left, node.right\n",
    "\n",
    "def gini_impurity(labels):\n",
    "    N_l = labels.shape[0]\n",
    "    return N_l * (1 - np.sum(np.square((np.unique(labels)[:, None] == labels).sum(axis=1)) / (N_l ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Also very similar to sample solution</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_decision_leaf_node(node):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    '''\n",
    "    # compute and store leaf response\n",
    "    node.N = node.data.shape[0]\n",
    "    most_frequent_label = np.argmax(np.bincount(node.labels))\n",
    "    node.response = most_frequent_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">We return a single label, whereas the sample solution returns a probability distribution</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def node_is_pure(node):\n",
    "    '''\n",
    "    check if 'node' ontains only instances of the same digit\n",
    "    '''\n",
    "    return len(np.unique(node.labels)) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# read and prepare the digits data\n",
    "digits = load_digits()\n",
    "data = digits.data\n",
    "images = digits.images\n",
    "target = digits.target\n",
    "target_names = digits.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train trees, plot training error confusion matrices, and comment on your results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_trees = [DensityTree().train(data[target == label], len(data[target == label]) / len(data), 10) \n",
    "                     for label in np.unique(target)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_predictions = np.array([[tree.predict(row) for tree in density_trees] for row in data]).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error rate: 40.62%\n",
      "compared to 90% error for pure guessing this is not too bad :)\n",
      "\n",
      "\n",
      "\t Predicted 0 | Predicted not 0\n",
      "\t -------------------------------\n",
      "Is 0     |  6.96%   |  2.95             \n",
      "Is not 0 |  0.00%   |  90.09             \n",
      "\n",
      "\n",
      "\t Predicted 1 | Predicted not 1\n",
      "\t -------------------------------\n",
      "Is 1     |  5.56%   |  4.56             \n",
      "Is not 1 |  7.51%   |  82.36             \n",
      "\n",
      "\n",
      "\t Predicted 2 | Predicted not 2\n",
      "\t -------------------------------\n",
      "Is 2     |  6.57%   |  3.28             \n",
      "Is not 2 |  5.34%   |  84.81             \n",
      "\n",
      "\n",
      "\t Predicted 3 | Predicted not 3\n",
      "\t -------------------------------\n",
      "Is 3     |  5.68%   |  4.51             \n",
      "Is not 3 |  3.06%   |  86.76             \n",
      "\n",
      "\n",
      "\t Predicted 4 | Predicted not 4\n",
      "\t -------------------------------\n",
      "Is 4     |  7.57%   |  2.50             \n",
      "Is not 4 |  5.45%   |  84.47             \n",
      "\n",
      "\n",
      "\t Predicted 5 | Predicted not 5\n",
      "\t -------------------------------\n",
      "Is 5     |  7.85%   |  2.28             \n",
      "Is not 5 |  10.29%   |  79.58             \n",
      "\n",
      "\n",
      "\t Predicted 6 | Predicted not 6\n",
      "\t -------------------------------\n",
      "Is 6     |  4.84%   |  5.23             \n",
      "Is not 6 |  0.00%   |  89.93             \n",
      "\n",
      "\n",
      "\t Predicted 7 | Predicted not 7\n",
      "\t -------------------------------\n",
      "Is 7     |  6.79%   |  3.17             \n",
      "Is not 7 |  3.45%   |  86.59             \n",
      "\n",
      "\n",
      "\t Predicted 8 | Predicted not 8\n",
      "\t -------------------------------\n",
      "Is 8     |  1.95%   |  7.74             \n",
      "Is not 8 |  3.06%   |  87.26             \n",
      "\n",
      "\n",
      "\t Predicted 9 | Predicted not 9\n",
      "\t -------------------------------\n",
      "Is 9     |  5.62%   |  4.40             \n",
      "Is not 9 |  2.45%   |  87.53             \n"
     ]
    }
   ],
   "source": [
    "print(f\"training error rate: {100 * sum(density_predictions != target) / len(target):.2f}%\")\n",
    "print(\"compared to 90% error for pure guessing this is not too bad :)\")\n",
    "\n",
    "for label in range(10):\n",
    "    print(\"\\n\")\n",
    "    print(f\"\\t Predicted {label} | Predicted not {label}\")\n",
    "    print(\"\\t -------------------------------\")\n",
    "    print(f\"Is {label}     |  {100 * sum(density_predictions[target == label] == label) / len(target):.2f}%   |  {100 * sum(density_predictions[target == label] != label)/ len(target):.2f}             \")\n",
    "    print(f\"Is not {label} |  {100 * sum(density_predictions[target != label] == label)/ len(target):.2f}%   |  {100 * sum(density_predictions[target != label] != label)/ len(target):.2f}             \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\t Predicted 0 | Predicted not 0\n",
      "\t -------------------------------\n",
      "Is 0     |  9.46%   |  0.45             \n",
      "Is not 0 |  1.17%   |  88.93             \n",
      "\n",
      "\n",
      "\t Predicted 1 | Predicted not 1\n",
      "\t -------------------------------\n",
      "Is 1     |  9.29%   |  0.83             \n",
      "Is not 1 |  2.23%   |  87.65             \n",
      "\n",
      "\n",
      "\t Predicted 2 | Predicted not 2\n",
      "\t -------------------------------\n",
      "Is 2     |  8.35%   |  1.50             \n",
      "Is not 2 |  0.72%   |  89.43             \n",
      "\n",
      "\n",
      "\t Predicted 3 | Predicted not 3\n",
      "\t -------------------------------\n",
      "Is 3     |  8.57%   |  1.61             \n",
      "Is not 3 |  1.00%   |  88.81             \n",
      "\n",
      "\n",
      "\t Predicted 4 | Predicted not 4\n",
      "\t -------------------------------\n",
      "Is 4     |  8.68%   |  1.39             \n",
      "Is not 4 |  0.67%   |  89.26             \n",
      "\n",
      "\n",
      "\t Predicted 5 | Predicted not 5\n",
      "\t -------------------------------\n",
      "Is 5     |  8.85%   |  1.28             \n",
      "Is not 5 |  1.28%   |  88.59             \n",
      "\n",
      "\n",
      "\t Predicted 6 | Predicted not 6\n",
      "\t -------------------------------\n",
      "Is 6     |  9.13%   |  0.95             \n",
      "Is not 6 |  0.17%   |  89.76             \n",
      "\n",
      "\n",
      "\t Predicted 7 | Predicted not 7\n",
      "\t -------------------------------\n",
      "Is 7     |  8.79%   |  1.17             \n",
      "Is not 7 |  1.61%   |  88.43             \n",
      "\n",
      "\n",
      "\t Predicted 8 | Predicted not 8\n",
      "\t -------------------------------\n",
      "Is 8     |  7.62%   |  2.06             \n",
      "Is not 8 |  2.23%   |  88.09             \n",
      "\n",
      "\n",
      "\t Predicted 9 | Predicted not 9\n",
      "\t -------------------------------\n",
      "Is 9     |  8.85%   |  1.17             \n",
      "Is not 9 |  1.34%   |  88.65             \n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTree().train(data, target)\n",
    "decision_predictions = np.array([decision_tree.predict(row) for row in data])\n",
    "\n",
    "for label in range(10):\n",
    "    print(\"\\n\")\n",
    "    print(f\"\\t Predicted {label} | Predicted not {label}\")\n",
    "    print(\"\\t -------------------------------\")\n",
    "    print(f\"Is {label}     |  {100 * sum(decision_predictions[target == label] == label) / len(target):.2f}%   |  {100 * sum(decision_predictions[target == label] != label)/ len(target):.2f}             \")\n",
    "    print(f\"Is not {label} |  {100 * sum(decision_predictions[target != label] == label)/ len(target):.2f}%   |  {100 * sum(decision_predictions[target != label] != label)/ len(target):.2f}             \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error rate: 12.41%\n",
      "this is a lot better than the density predictions. It probably helps if an algorithm \"knows\" about the training labels\n"
     ]
    }
   ],
   "source": [
    "print(f\"training error rate: {100 * sum(decision_predictions != target) / len(target):.2f}%\")\n",
    "print(\"this is a lot better than the density predictions. It probably helps if an algorithm \\\"knows\\\" about the training labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DensityForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DensityTree() for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, data, prior, n_min=20):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "            data_selection_indices = np.random.choice(data.shape[0], size=data.shape[0], replace=True)\n",
    "            tree.train(data[data_selection_indices], prior, n_min)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the ensemble prediction\n",
    "        return sum([tree.predict(x) for tree in self.trees]) / len(self.trees) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Added replace=True attribute, but it is true by default.</div>\n",
    "<div style=\"color: green; font-weight: bold\">The mean was computed manually.</div>\n",
    "<div style=\"color: green; font-weight: bold\">Otherwise very similar to sample solution.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DecisionForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DecisionTree() for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, data, labels, n_min=0):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "            data_selection_indices = np.random.choice(data.shape[0], size=data.shape[0], replace=True)\n",
    "            tree.train(data[data_selection_indices], labels[data_selection_indices], n_min)\n",
    "    def predict(self, x):\n",
    "        # compute the ensemble prediction\n",
    "        predictions = np.array([tree.predict(x) for tree in self.trees])\n",
    "        return np.argmax(np.bincount(predictions))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">We return an individual label as prediction, whereas the sample solution returns a probability distribution.</div>\n",
    "<div style=\"color: green; font-weight: bold\">Otherwise very similar to sample solution</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-517421b96c46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDensityForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mforests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-58985990def6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, prior, n_min)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;31m# train each tree, using a bootstrap sample of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mdata_selection_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_selection_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-21d19785d022>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, prior, n_min)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# and returns the two children, which must be placed on the 'stack'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mfeature_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mD_try\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_density_split_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0mstack\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9022d669a0c3>\u001b[0m in \u001b[0;36mmake_density_split_node\u001b[0;34m(node, N, feature_indices)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             loo_error = ( leave_one_out_error(node.data, node.data[node.data[:, feat] <= t], t, N)\n\u001b[0;32m---> 25\u001b[0;31m                         + leave_one_out_error(node.data, node.data[node.data[:, feat] > t], t, N))\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# choose the best threshold that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9022d669a0c3>\u001b[0m in \u001b[0;36mleave_one_out_error\u001b[0;34m(data, data_below_threshold, t, N)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mN_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_below_threshold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mM_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_below_threshold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mm_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_below_threshold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     38\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     39\u001b[0m           initial=_NoValue, where=True):\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train forests (with 20 trees per forest), plot training error confusion matrices, and comment on your results\n",
    "digits = load_digits()\n",
    "\n",
    "densityForests = []\n",
    "for i in range(10):\n",
    "    data = digits.data[digits.target == i]\n",
    "    prior = data.shape[0] / digits.data.shape[0]\n",
    "    forest = DensityForest(20)\n",
    "    forest.train(data, prior)\n",
    "    densityForests.append(forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictDensityForest(x):\n",
    "    max_response = float(\"inf\")\n",
    "    label = None\n",
    "    for (index, forest) in enumerate(densityForests):\n",
    "        response = forest.predict(x)\n",
    "        print(response)\n",
    "        if response > max_response:\n",
    "            max_response = response\n",
    "            label = index\n",
    "    return label\n",
    "\n",
    "print(predictDensityForest(digits.data[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">We did not finish the evaluation task.</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
