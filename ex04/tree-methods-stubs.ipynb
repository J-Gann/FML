{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base classes\n",
    "\n",
    "class Node:\n",
    "    pass\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "    \n",
    "    def find_leaf(self, x):\n",
    "        node = self.root\n",
    "        while hasattr(node, \"feature\"):\n",
    "            j = node.feature\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DensityTree, self).__init__()\n",
    "        \n",
    "    def train(self, data, prior, n_min=20):\n",
    "        '''\n",
    "        data: the feature matrix for the digit under consideration\n",
    "        prior: the prior probability of this digit\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        self.prior = prior\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D)) # number of features to consider for each split decision\n",
    "\n",
    "        # find and remember the tree's bounding box, \n",
    "        # i.e. the lower and upper limits of the training feature set\n",
    "        m, M = np.min(data, axis=0), np.max(data, axis=0)\n",
    "        self.box = m.copy(), M.copy()\n",
    "        \n",
    "        # identify invalid features and adjust the bounding box\n",
    "        # (If m[j] == M[j] for some j, the bounding box has zero volume, \n",
    "        #  causing divide-by-zero errors later on. We must exclude these\n",
    "        #  features from splitting and adjust the bounding box limits \n",
    "        #  such that invalid features have no effect on the volume.)\n",
    "        valid_features   = np.where(m != M)[0]\n",
    "        invalid_features = np.where(m == M)[0]\n",
    "        M[invalid_features] = m[invalid_features] + 1\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.box = m.copy(), M.copy()\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0] # number of instances in present node\n",
    "            if n >= n_min:\n",
    "\n",
    "                # Call 'make_density_split_node()' with 'D_try' randomly selected \n",
    "                # indices from 'valid_features'. This turns 'node' into a split node\n",
    "                # and returns the two children, which must be placed on the 'stack'.\n",
    "                feature_indices = np.random.permutation(np.arange(D))[:D_try]\n",
    "                left, right = make_density_split_node(node, N, feature_indices)\n",
    "                stack += [left, right]\n",
    "            else:\n",
    "                # Call 'make_density_leaf_node()' to turn 'node' into a leaf node.\n",
    "                make_density_leaf_node(node, N)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        m, M = self.box\n",
    "        \n",
    "        if np.any(x > M) or np.any(x < m):\n",
    "            return 0\n",
    "\n",
    "        leaf = self.find_leaf(x)\n",
    "        # return p(x | y) * p(y) if x is within the tree's bounding box \n",
    "        # and return 0 otherwise\n",
    "        return leaf.response * self.prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_density_split_node(node, N, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    N:    the total number of training instances for the current class\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "    m, M = node.box\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    e_min = float(\"inf\")\n",
    "    \n",
    "    for feat in feature_indices:\n",
    "        # Hint: For each feature considered, first remove duplicate feature values using \n",
    "        # 'np.unique()'. Describe here why this is necessary.\n",
    "        # It is necessary because otherwise we would recieve a threshold candidates\n",
    "        # on a feature value not between two of them\n",
    "        data_unique = np.sort(np.unique(node.data[:, feat]), axis=0)\n",
    "        # Compute candidate thresholds\n",
    "        tj = (data_unique[1:] + data_unique[:-1]) / 2\n",
    "        \n",
    "        for t in tj:\n",
    "            loo_error = ( leave_one_out_error(node.data, node.data[node.data[:, feat] <= t], t, N)\n",
    "                        + leave_one_out_error(node.data, node.data[node.data[:, feat] > t], t, N))\n",
    "\n",
    "            # choose the best threshold that\n",
    "            if loo_error < e_min:\n",
    "                e_min = loo_error\n",
    "                node.feature = feat\n",
    "                node.threshold = t\n",
    "\n",
    "    # create children\n",
    "    node.left = Node()\n",
    "    node.right = Node()\n",
    "    \n",
    "    # initialize 'left' and 'right' with the data subsets and bounding boxes\n",
    "    # according to the optimal split found above\n",
    "    is_left = node.data[:, node.feature] < node.threshold\n",
    "    node.left.data = node.data[is_left]\n",
    "    node.left.box = (node.left.data.min(axis=0), node.left.data.max(axis=0))\n",
    "    node.right.data = node.data[~is_left]\n",
    "    node.right.box = (node.right.data.min(axis=0), node.right.data.max(axis=0))\n",
    "    \n",
    "    return node.left, node.right\n",
    "\n",
    "\n",
    "def leave_one_out_error(data, data_below_threshold, t, N):\n",
    "    N_l = data_below_threshold.shape[0]\n",
    "    \n",
    "    M_l = data_below_threshold.max(axis=0)\n",
    "    m_l = data_below_threshold.min(axis=0)\n",
    "    \n",
    "    # sadly this also works to fix the box of the node\n",
    "    M_l += M_l == m_l\n",
    "\n",
    "    V_l = np.product(M_l - m_l)\n",
    "    return (N_l / (N * V_l)) * ((N_l / N) - 2 * (N_l - 1) / (N - 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_density_leaf_node(node, N):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    N:    the total number of training instances for the current class\n",
    "    '''\n",
    "    # compute and store leaf response\n",
    "    n = node.data.shape[0]\n",
    "    node.response = n / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DecisionTree, self).__init__()\n",
    "        \n",
    "    def train(self, data, labels, n_min=20):\n",
    "        '''\n",
    "        data: the feature matrix for all digits\n",
    "        labels: the corresponding ground-truth responses\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D)) # how many features to consider for each split decision\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.labels = labels\n",
    "        \n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0] # number of instances in present node\n",
    "            if n >= n_min and not node_is_pure(node):\n",
    "                # Call 'make_decision_split_node()' with 'D_try' randomly selected \n",
    "                # feature indices. This turns 'node' into a split node\n",
    "                # and returns the two children, which must be placed on the 'stack'.\n",
    "                feature_indices = np.random.permutation(np.arange(D))[:D_try]\n",
    "                left, right = make_decision_split_node(node, feature_indices)\n",
    "                stack += [left, right]\n",
    "            else:\n",
    "                # Call 'make_decision_leaf_node()' to turn 'node' into a leaf node.\n",
    "                make_decision_leaf_node(node)\n",
    "        \n",
    "        return self\n",
    "                \n",
    "    def predict(self, x):\n",
    "        leaf = self.find_leaf(x)\n",
    "        # compute p(y | x)\n",
    "        return leaf.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decision_split_node(node, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    e_min = float(\"inf\")\n",
    "    node.feature = None\n",
    "    node.threshold = None\n",
    "    \n",
    "    for feat in feature_indices:\n",
    "        data_unique = np.sort(np.unique(node.data[:, feat]), axis=0)\n",
    "\n",
    "        # Compute candidate thresholds\n",
    "        tj = (data_unique[1:] + data_unique[:-1]) / 2\n",
    "        \n",
    "        for t in tj:\n",
    "            gi_error = (gini_impurity(node.labels[node.data[:, feat] <= t])\n",
    "                      + gini_impurity(node.labels[node.data[:, feat] > t]))\n",
    "            \n",
    "            if gi_error < e_min:\n",
    "                e_min = gi_error\n",
    "                node.feature = feat\n",
    "                node.threshold = t\n",
    "    \n",
    "    # create children\n",
    "    node.left = Node()\n",
    "    node.right = Node()\n",
    "    \n",
    "    # initialize 'left' and 'right' with the data subsets and labels\n",
    "    # according to the optimal split found above    \n",
    "    is_left = node.data[:, node.feature] < node.threshold\n",
    "    node.left.data = node.data[is_left]\n",
    "    node.left.labels = node.labels[is_left]\n",
    "    node.right.data = node.data[~is_left]\n",
    "    node.right.labels = node.labels[~is_left]\n",
    "    \n",
    "    # return the children (to be placed on the stack)\n",
    "    return node.left, node.right\n",
    "\n",
    "def gini_impurity(labels):\n",
    "    N_l = labels.shape[0]\n",
    "    return N_l * (1 - np.sum(np.square((np.unique(labels)[:, None] == labels).sum(axis=1)) / (N_l ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_decision_leaf_node(node):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    '''\n",
    "    # compute and store leaf response\n",
    "    node.N = node.data.shape[0]\n",
    "    most_frequent_label = np.argmax(np.bincount(node.labels))\n",
    "    node.response = most_frequent_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def node_is_pure(node):\n",
    "    '''\n",
    "    check if 'node' ontains only instances of the same digit\n",
    "    '''\n",
    "    return len(np.unique(node.labels)) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# read and prepare the digits data\n",
    "digits = load_digits()\n",
    "data = digits.data\n",
    "images = digits.images\n",
    "target = digits.target\n",
    "target_names = digits.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train trees, plot training error confusion matrices, and comment on your results\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DensityTree at 0x7f07087bfc70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DensityTree()\n",
    "threes = data[target == 3]\n",
    "tree.train(threes, len(threes) / len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_trees = [DensityTree().train(data[target == label], len(data[target == label]) / len(data), 10) \n",
    "                     for label in np.unique(target)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_predictions = np.array([[tree.predict(row) for tree in density_trees] for row in data]).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error rate: 35.39%\n",
      "compared to 90% error for pure guessing this is not too bad :)\n"
     ]
    }
   ],
   "source": [
    "print(f\"training error rate: {100 * sum(density_predictions != target) / len(target):.2f}%\")\n",
    "print(\"compared to 90% error for pure guessing this is not too bad :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTree().train(data, target)\n",
    "decision_predictions = np.array([decision_tree.predict(row) for row in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error rate: 13.36%\n",
      "this is a lot better than the density predictions. It probably helps if an algorithm \"knows\" about the training labels\n"
     ]
    }
   ],
   "source": [
    "print(f\"training error rate: {100 * sum(decision_predictions != target) / len(target):.2f}%\")\n",
    "print(\"this is a lot better than the density predictions. It probably helps if an algorithm \\\"knows\\\" about the training labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DensityForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DensityTree() for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, data, prior, n_min=20):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "            data_selection_indices = np.random.choice(data.shape[0], size=data.shape[0], replace=True)\n",
    "            tree = DensityTree()\n",
    "            tree.train(data[data_selection_indices], prior, n_min)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the ensemble prediction\n",
    "        return sum([tree.predict(x) for tree in self.trees]) / len(self.trees) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DecisionForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DecisionTree() for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, data, labels, n_min=0):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "            data_selection_indices = np.random.choice(data.shape[0], size=data.shape[0], replace=True)\n",
    "            tree = DensityTree()\n",
    "            tree.train(data[data_selection_indices], labels[data_selection_indices], n_min)\n",
    "    def predict(self, x):\n",
    "        # compute the ensemble prediction\n",
    "        predictions = np.array([tree.predict(x) for tree in self.trees])\n",
    "        return np.argmax(np.bincount(predictions))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-5c7300cfe929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDensityForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mforests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-72d77a24b67c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, prior, n_min)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mdata_selection_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDensityTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_selection_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-21d19785d022>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, prior, n_min)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# and returns the two children, which must be placed on the 'stack'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mfeature_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mD_try\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_density_split_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0mstack\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9022d669a0c3>\u001b[0m in \u001b[0;36mmake_density_split_node\u001b[0;34m(node, N, feature_indices)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             loo_error = ( leave_one_out_error(node.data, node.data[node.data[:, feat] <= t], t, N)\n\u001b[0m\u001b[1;32m     25\u001b[0m                         + leave_one_out_error(node.data, node.data[node.data[:, feat] > t], t, N))\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9022d669a0c3>\u001b[0m in \u001b[0;36mleave_one_out_error\u001b[0;34m(data, data_below_threshold, t, N)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mM_l\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mM_l\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mm_l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mV_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM_l\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mm_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN_l\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mV_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_l\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN_l\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mproduct\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mproduct\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3742\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prod_dispatcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3743\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3744\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train forests (with 20 trees per forest), plot training error confusion matrices, and comment on your results\n",
    "# train forests (with 20 trees per forest), plot training error confusion matrices, and comment on your results\n",
    "digits = load_digits()\n",
    "\n",
    "forests = []\n",
    "for i in range(10):\n",
    "    data = digits.data[digits.target == i]\n",
    "    prior = data.shape[0] / digits.data.shape[0]\n",
    "    forest = DensityForest(20)\n",
    "    forest.train(data, prior)\n",
    "    forests.append(forest)\n",
    "\n",
    "def predict(x):\n",
    "    max_response = float(\"inf\")\n",
    "    label = None\n",
    "    for (index, forest) in enumerate(forests):\n",
    "        response = forest.predict(x)\n",
    "        print(response)\n",
    "        if response < max_response:\n",
    "            max_response = response\n",
    "            label = index\n",
    "\n",
    "predict(digits.data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
